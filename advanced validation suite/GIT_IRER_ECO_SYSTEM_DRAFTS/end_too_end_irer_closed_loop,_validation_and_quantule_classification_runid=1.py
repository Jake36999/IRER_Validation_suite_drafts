# -*- coding: utf-8 -*-
"""END-TOO_END IRER Closed Loop, validation and quantule classification RunID=1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SBZVOYN0Uc3J1lE1CkJs7-zPSrRXjAOO

Here is a complete `README.md` file to run the ASTE ecosystem.

-----

# ASTE: Adaptive Simulation Steering Engine (End-to-End Closed Loop)

This repository contains the complete, end-to-end closed-loop system for the Adaptive Simulation Steering Engine (ASTE). This system is designed to autonomously run physics simulations, validate the scientific results, and use an evolutionary algorithm to "hunt" for parameters that optimize for scientific fidelity.

This system is the primary engineering implementation of the "Differentiable Governance" and "Immutable AI" framework.

## ðŸ“¦ Core Components

The ecosystem consists of 5 critical Python scripts that must all be in the same directory:

1.  **`adaptive_hunt_orchestrator.py` (The Driver):** The main entry point for the entire system. It coordinates all other components to execute the full "Adaptive Hunt Loop".
2.  **`aste_hunter.py` (The "Brain"):** Manages the `simulation_ledger.csv`. It reads validation reports, calculates fitness (`1 / SSE`), and breeds the next generation of parameters.
3.  **`worker_v7.py` (The "Worker"):** The JAX-based physics engine. It runs a single simulation and is architected with `jax.lax.scan` to resolve HPC compilation errors.
4.  **`validation_pipeline.py` (The "Validator" / Asset A6):** The Spectral Fidelity & Provenance (SFP) module. It runs the scientific analysis, generates the `config_hash`, and calculates the final `log_prime_sse` and Aletheia Coherence Metrics (PCS, PLI, IC).
5.  **`quantulemapper.py` (The "Profiler" / CEPP v1.0):** The core scientific analysis engine. This is a dependency called by the `validation_pipeline.py` to perform the actual multi-ray sampling, FFT, and classification.

-----

## ðŸš€ How to Run

Follow these steps to set up the environment and execute the full, closed-loop simulation.

### 1\. Setup Environment

You must have all 5 Python scripts listed above in the same directory.

### 2\. Install Dependencies

This system requires `jax`, `h5py`, `scipy`, and `pandas`.

```bash
pip install jax jaxlib h5py scipy pandas
```

### 3\. Clean Workspace (Optional, but Recommended)

To ensure a fresh run, delete any artifacts from previous tests:

```bash
rm -f *.csv *.json *.h5
rm -rf input_configs simulation_data provenance_reports
```

### 4\. Execute the System

Run the main orchestrator script. This will start the entire end-to-end loop.

```bash
python adaptive_hunt_orchestrator.py
```

-----

## ðŸ“Š What to Expect

When you run the command, you will see a detailed log of the entire process:

1.  **Bootstrap:** The orchestrator will start and create the required directories (`input_configs`, `simulation_data`, `provenance_reports`).
2.  **Generation 0:** The `aste_hunter` will initialize, see no `simulation_ledger.csv`, and generate a random set of parameters for "Generation 0".
3.  **Job Execution:** The orchestrator will loop through each parameter set (e.g., 4 jobs for Generation 0). For each job, you will see:
      * `[Orchestrator] -> Calling Worker: worker_v7.py`
      * (JAX compilation will happen here on the first run, which may take a minute)
      * `[Orchestrator] <- Worker ... OK.`
      * `[Orchestrator] -> Calling Validator: validation_pipeline.py`
      * (The validator will call the `quantulemapper` and run all analyses)
      * `[Orchestrator] <- Validator ... OK.`
4.  **Generation 1:** After Generation 0 is complete, the `aste_hunter` will read all the new `provenance.json` files, calculate their fitness, and breed a new set of parameters for "Generation 1".
5.  **Loop:** The system will repeat Step 3 for all jobs in Generation 1.
6.  **Completion:** The script will finish and print **"--- ASTE ORCHESTRATOR: ALL GENERATIONS COMPLETE ---"**.

### Final Artifacts

After the run, your directory will be populated with all the mandated artifacts, proving the system's "Immutable Provenance":

  * **`simulation_ledger.csv`:** The master log containing all runs (e.g., 8 runs for 2 generations) with their `log_prime_sse` and `fitness` scores populated.
  * **`input_configs/`:** Contains all `config_...json` files, one for each run.
  * **`simulation_data/`:** Contains all `rho_history_...h5` files from the worker.
  * **`provenance_reports/`:** Contains the "birth certificates":
      * `provenance_...json`: The final validation report for each run.
      * `..._quantule_events.csv`: The Quantule Atlas data generated by the profiler.
      Here is a set of diagnostics and tips for running the final, end-to-end notebook.

-----

## ðŸš€ Setup and Execution

1.  **Environment:** Ensure your Colab (or local) environment has all dependencies installed.
    ```bash
    !pip install jax jaxlib h5py scipy pandas
    ```
2.  **Run All `%%writefile` Cells:** The notebook is designed to be self-contained. It writes all 5 component scripts (`aste_hunter.py`, `worker_v7.py`, etc.) to the virtual machine's disk. You **must run these cells first** so the files exist.
3.  **Clean Workspace:** Before starting the main orchestrator, run the cleanup cell to delete old artifacts (`*.csv`, `*.h5`, `*.json`). This ensures your `simulation_ledger.csv` isn't polluted by old, failed runs.
4.  **Execute the Orchestrator:** Run the final cell, which calls:
    ```bash
    !python adaptive_hunt_orchestrator.py
    ```

-----

## ðŸ©º Quick Diagnostics and Common Tracebacks

Here are the most likely tracebacks you'll see, their cause, and the fix.

### 1\. `ImportError` or `FileNotFoundError` (The "File is Missing" Error)

  * **Traceback (in Orchestrator):** `ImportError: No module named 'aste_hunter'` or `ImportError: No module named 'quantulemapper'`.
  * **Traceback (in Subprocess Log):** `FileNotFoundError: [Errno 2] No such file or directory: 'worker_v7.py'`.
  * **Diagnostic:** This is the most common setup failure. It means the Python script file that the orchestrator or validator is trying to import or run does not exist on the disk.
  * **Tipbit:** You **must** run all the `%%writefile` cells at the top of the notebook to create the `.py` files *before* you run the main orchestrator script.

-----

### 2\. JAX `TypeError` (The "HPC Gap" Error)

  * **Traceback (in Worker Log):** `TypeError: Non-hashable static arguments are not supported` or `jax.errors.ConcretizationTypeError`.
  * **Diagnostic:** This is the critical **HPC Gap**. It means a JAX array (like `k_vectors` or `k_squared`) is being passed as a static argument to a JIT-compiled function instead of being managed as dynamic state.
  * **Tipbit:** This error indicates a regression in our `worker_v7.py` script. The **mandated solution** is to ensure all JAX arrays are bundled *inside* the `SimState` `NamedTuple` and passed as the `carry` variable to `jax.lax.scan`. Check that no JAX arrays are being passed as `static_argnames` or closed over from the global scope.

-----

### 3\. `KeyError` (The "Data Contract" Error)

  * **Traceback (in Hunter Log):** `KeyError: 'log_prime_sse'` or `KeyError: 'spectral_fidelity'`.
  * **Traceback (in Orchestrator Log):** `KeyError: 'param_kappa'`.
  * **Diagnostic:** The "data contract" between the components is broken. The `validation_pipeline.py` produced a `provenance.json` with a structure that `aste_hunter.py` did not expect.
  * **Tipbit:** Manually open the `provenance_...json` file from the `provenance_reports` directory. Compare its structure to the `provenance_artifact` dictionary defined in `validation_pipeline.py` and the parsing logic in `aste_hunter.py`'s `process_generation_results` function.

-----

### 4\. `FileNotFoundError` (The "Workflow" Error)

  * **Traceback (in Validator Log):** `FileNotFoundError: [Errno 2] No such file or directory: 'simulation_data/rho_history_...h5'`.
  * **Traceback (in Hunter Log):** `FileNotFoundError: [Errno 2] No such file or directory: 'provenance_reports/provenance_...json'`.
  * **Diagnostic:** This is a **workflow dependency failure**. The `worker_v7.py` failed (likely with the JAX `TypeError` above) and never created its HDF5 artifact. This caused the `validation_pipeline.py` to fail, which in turn caused the `aste_hunter.py` to fail.
  * **Tipbit:** **Always read the logs from top to bottom.** The *last* error (e.g., in the Hunter) is almost always caused by an *earlier* error (e.g., in the Worker).

-----

## ðŸ’¡ Final Tipbits

  * **Be Patient with JAX:** The *very first job* of Generation 0 will be slow (30-60 seconds). This is **normal**. It's JAX JIT-compiling the entire `worker_v7.py` physics loop. All subsequent jobs will be extremely fast (1-2 seconds).
  * **Check the Ledger:** The ultimate source of truth is the `simulation_ledger.csv` file. After the run, download and open it. If it contains 8 rows (for 2 generations of 4) and all `log_prime_sse` and `fitness` columns are populated, the entire end-to-end run was a **100% success**.

ASTE System Setup & Test Guide
This guide details how to run the complete, closed-loop END-TOO_END IRER Closed Loop, validation and quantule classification RunID=1.ipynb notebook.

1. Setup Instructions
The notebook is designed to be self-contained. You must run the cells in the correct order.

Step 1: Install Dependencies Run the first code cell to install all required Python libraries:

Bash

!pip install jax jaxlib h5py scipy pandas
Step 2: Create All Component Scripts Run all the %%writefile cells. This will create the 5 core scripts in your Colab environment:

validation_pipeline.py (The Validator)

quantulemapper.py (The Profiler)

aste_hunter.py (The "Brain")

worker_v7.py (The "Worker")

adaptive_hunt_orchestrator.py (The "Driver")

Step 3: Clean the Workspace Run the cleanup cell to remove any artifacts from previous test runs. This ensures you are starting with a fresh simulation_ledger.csv.

Bash

!rm -f *.csv *.json *.h5
!rm -rf input_configs simulation_data provenance_reports
2. Running the Test
Step 4: Execute the Full System Run the final code cell, which executes the master driver script:

Bash

!python adaptive_hunt_orchestrator.py
3. What to Expect
This command starts the entire end-to-end adaptive loop.

Initial JAX Compilation: The first job of Generation 0 will be slow (e.g., 30-60 seconds). This is normal. It is JAX JIT-compiling the entire worker_v7.py physics loop. All subsequent jobs will be much faster.

Console Log: You will see a detailed log as the orchestrator runs:

The system will boot, create directories, and the Hunter will initialize.

The Hunter will create Generation 0 (e.g., 4 random parameter sets).

The Orchestrator will loop through each of these 4 jobs, printing:

[Orchestrator] -> Calling Worker: worker_v7.py

[Orchestrator] <- Worker ... OK.

[Orchestrator] -> Calling Validator: validation_pipeline.py

[Orchestrator] <- Validator ... OK.

The Hunter will process the results and print the best SSE from Generation 0.

The Hunter will then breed Generation 1.

The Orchestrator will repeat step 3 for all 4 jobs in Generation 1.

The script will finish with "--- ASTE ORCHESTRATOR: ALL GENERATIONS COMPLETE ---".

Final Artifacts: After the run, the ultimate proof of success is in the files created:

simulation_ledger.csv: This is the master ledger. It should contain 8 rows (4 for Gen 0, 4 for Gen 1), and all log_prime_sse and fitness columns should be fully populated.

provenance_reports/: This directory will be full of artifacts:

8 provenance_...json files (the "birth certificates").

8 ..._quantule_events.csv files (the Quantule Atlas data).

simulation_data/: Will contain 8 rho_history_...h5 files from the worker.

input_configs/: Will contain 8 config_...json files.

****************************************
#AI LOG START
"""

# Step 3: Clean the Workspace
!rm -f *.csv *.json *.h5
!rm -rf input_configs simulation_data provenance_reports

# Commented out IPython magic to ensure Python compatibility.
# %%writefile validation_pipeline.py
# #!/usr/bin/env python3
# 
# """
# validation_pipeline.py
# ASSET: A6 (Spectral Fidelity & Provenance Module)
# VERSION: 2.0 (Phase 3 Scientific Mandate)
# CLASSIFICATION: Final Implementation Blueprint / Governance Instrument
# GOAL: Serves as the immutable source of truth that cryptographically binds
#       experimental intent (parameters) to scientific fact (spectral fidelity)
#       and Aletheia cognitive coherence.
# """
# 
# import json
# import hashlib
# import sys
# import os
# import argparse
# import h5py
# import numpy as np
# import pandas as pd
# from datetime import datetime, timezone
# from typing import Dict, Any, List, Tuple
# import tempfile # Added for temporary file handling
# 
# # --- V2.0 DEPENDENCIES ---
# # Import the core analysis engine (CEPP v1.0 / Quantule Profiler)
# # This file (quantulemapper.py) must be in the same directory.
# try:
#     import quantulemapper as cep_profiler
# except ImportError:
#     print("FATAL: Could not import 'quantulemapper.py'.", file=sys.stderr)
#     print("This file is the core Quantule Profiler (CEPP v1.0).", file=sys.stderr)
#     sys.exit(1)
# 
# # Import Scipy for new Aletheia Metrics
# try:
#     from scipy.signal import coherence as scipy_coherence
#     from scipy.stats import entropy as scipy_entropy
# except ImportError:
#     print("FATAL: Missing 'scipy'. Please install: pip install scipy", file=sys.stderr)
#     sys.exit(1)
# 
# 
# # --- MODULE CONSTANTS ---
# SCHEMA_VERSION = "SFP-v2.0-ARCS" # Upgraded schema version
# 
# # ---
# # SECTION 1: PROVENANCE KERNEL (EVIDENTIAL INTEGRITY)
# # ---
# 
# def generate_canonical_hash(params_dict: Dict[str, Any]) -> str:
#     """
#     Generates a canonical, deterministic SHA-256 hash from a parameter dict.
#     This function now explicitly filters out non-canonical metadata like 'run_uuid' and 'config_hash'
#     to ensure consistency across components.
#     """
#     try:
#         # Create a filtered dictionary for hashing, excluding non-canonical keys
#         filtered_params = {k: v for k, v in params_dict.items() if k not in ["run_uuid", "config_hash", "param_hash_legacy"]}
# 
#         canonical_string = json.dumps(
#             filtered_params,
#             sort_keys=True,
#             separators=(
#                 ',', ':'
#             )
#         )
#         string_bytes = canonical_string.encode('utf-8')
#         hash_object = hashlib.sha256(string_bytes)
#         config_hash = hash_object.hexdigest()
#         return config_hash
#     except Exception as e:
#         print(f"[ProvenanceKernel Error] Failed to generate hash: {e}", file=sys.stderr)
#         raise
# 
# # ---
# # SECTION 2: FIDELITY KERNEL (SCIENTIFIC VALIDATION)
# # ---
# 
# def run_quantule_profiler(rho_history_path: str) -> Dict[str, Any]:
#     """
#     Orchestrates the core scientific analysis by calling the
#     Quantule Profiler (CEPP v1.0 / quantulemapper.py).
# 
#     This function replaces the v1.0 mock logic. It loads the HDF5 artifact,
#     saves it as a temporary .npy file (as required by the profiler's API),
#     and runs the full analysis.
#     """
#     temp_npy_file = None
#     try:
#         # 1. Load HDF5 data (as required by Orchestrator)
#         with h5py.File(rho_history_path, 'r') as f:
#             # Load the full 4D stack
#             rho_history = f['rho_history'][:]
# 
#         if rho_history.ndim != 4:
#             raise ValueError(f"Input HDF5 'rho_history' is not 4D (t,x,y,z). Shape: {rho_history.shape}")
# 
#         # 2. Convert to .npy (as required by quantulemapper.py API)
#         # We create a temporary .npy file for the profiler to consume
#         with tempfile.NamedTemporaryFile(suffix=".npy", delete=False) as tmp:
#             np.save(tmp, rho_history)
#             temp_npy_file = tmp.name
# 
#         # 3. Run the Quantule Profiler (CEPP v1.0)
#         # This performs: Multi-Ray Sampling, FFT, Peak Find, Calibration,
#         # SSE Calculation, and Quantule Classification.
#         print(f"[FidelityKernel] Calling Quantule Profiler (CEPP v1.0) on {temp_npy_file}")
# 
#         # The mapper.py analyze_4d function runs on the *last time step*
#         profiler_results = cep_profiler.analyze_4d(temp_npy_file)
# 
#         # 4. Extract key results for the SFP artifact
#         # The profiler already calculates SSE, which we now use as the
#         # definitive "log_prime_sse"
# 
#         spectral_fidelity = {
#             "validation_status": profiler_results.get("validation_status", "FAIL: PROFILER"),
#             "log_prime_sse": profiler_results.get("total_sse", 999.0),
#             "scaling_factor_S": profiler_results.get("scaling_factor_S", 0.0),
#             "dominant_peak_k_raw": profiler_results.get("dominant_peak_k", 0.0),
#             "analysis_protocol": "CEPP v1.0",
#             "log_prime_targets": cep_profiler.LOG_PRIME_VALUES.tolist()
#         }
# 
#         # Return the full set of results for the Aletheia Metrics
#         return {
#             "spectral_fidelity": spectral_fidelity,
#             "classification_results": profiler_results.get("csv_files", {}),
#             "raw_rho_final_state": rho_history[-1, :, :, :] # Pass final state
#         }
# 
#     except Exception as e:
#         print(f"[FidelityKernel Error] Failed during Quantule Profiler execution: {e}", file=sys.stderr)
#         return {
#             "spectral_fidelity": {"validation_status": "FAIL: KERNEL_ERROR", "log_prime_sse": 999.9},
#             "classification_results": {},
#             "raw_rho_final_state": None
#         }
#     finally:
#         # Clean up the temporary .npy file
#         if temp_npy_file and os.path.exists(temp_npy_file):
#             os.remove(temp_npy_file)
# 
# # ---
# # SECTION 3: ALETHEIA COHERENCE METRICS (PHASE 3)
# # ---
# 
# def calculate_pcs(rho_final_state: np.ndarray) -> float:
#     """
#     [Phase 3] Calculates the Phase Coherence Score (PCS).
#     Analogue: Superfluid order parameter.
#     Implementation: Magnitude-squared coherence function.
# 
#     We sample two different, parallel 1D rays from the final state
#     and measure their coherence.
#     """
#     try:
#         # Sample two 1D rays from the middle of the state
#         center_idx = rho_final_state.shape[0] // 2
#         ray_1 = rho_final_state[center_idx, center_idx, :]
#         ray_2 = rho_final_state[center_idx + 1, center_idx + 1, :] # Offset ray
# 
#         # Calculate coherence
#         f, Cxy = scipy_coherence(ray_1, ray_2)
# 
#         # PCS is the mean coherence across all frequencies
#         pcs_score = np.mean(Cxy)
# 
#         if np.isnan(pcs_score):
#             return 0.0
#         return float(pcs_score)
# 
#     except Exception as e:
#         print(f"[AletheiaMetrics] WARNING: PCS calculation failed: {e}", file=sys.stderr)
#         return 0.0 # Failed coherence is 0
# 
# def calculate_pli(rho_final_state: np.ndarray) -> float:
#     """
#     [Phase 3] Calculates the Principled Localization Index (PLI).
#     Analogue: Mott Insulator phase.
#     Implementation: Inverse Participation Ratio (IPR).
# 
#     IPR = sum(psi^4) / (sum(psi^2))^2
#     A value of 1.0 is perfectly localized (Mott), 1/N is perfectly delocalized (Superfluid).
#     We use the density field `rho` as our `psi^2` equivalent.
#     """
#     try:
#         # Normalize the density field (rho is already > 0)
#         # Changed jnp.sum to np.sum and jnp.array to np.array
#         rho_norm = rho_final_state / np.sum(rho_final_state)
# 
#         # Calculate IPR on the normalized density
#         # IPR = sum(p_i^2)
#         pli_score = np.sum(rho_norm**2)
# 
#         # Scale by N to get a value between (0, 1)
#         N_cells = rho_final_state.size
#         pli_score_normalized = float(pli_score * N_cells)
# 
#         if np.isnan(pli_score_normalized):
#             return 0.0
#         return pli_score_normalized
# 
#     except Exception as e:
#         print(f"[AletheiaMetrics] WARNING: PLI calculation failed: {e}", file=sys.stderr)
#         return 0.0
# 
# def calculate_ic(rho_final_state: np.ndarray) -> float:
#     """
#     [Phase 3] Calculates the Informational Compressibility (IC).
#     Analogue: Thermodynamic compressibility.
#     Implementation: K_I = dS / dE (numerical estimation).
#     """
#     try:
#         # 1. Proxy for System Energy (E):
#         # We use the L2 norm of the field (sum of squares) as a simple energy proxy.
#         # Changed jnp.sum to np.sum
#         proxy_E = np.sum(rho_final_state**2)
# 
#         # 2. Proxy for System Entropy (S):
#         # We treat the normalized field as a probability distribution
#         # and calculate its Shannon entropy.
#         rho_flat = rho_final_state.flatten()
#         rho_prob = rho_flat / np.sum(rho_flat)
#         # Add epsilon to avoid log(0)
#         proxy_S = scipy_entropy(rho_prob + 1e-9)
# 
#         # 3. Calculate IC = dS / dE
#         # We perturb the system slightly to estimate the derivative
# 
#         # Create a tiny perturbation (add 0.1% energy)
#         epsilon = 0.001
#         rho_perturbed = rho_final_state * (1.0 + epsilon)
# 
#         # Calculate new E and S
#         # Changed jnp.sum to np.sum
#         proxy_E_p = np.sum(rho_perturbed**2)
# 
#         rho_p_flat = rho_perturbed.flatten()
#         rho_p_prob = rho_p_flat / np.sum(rho_p_flat)
#         proxy_S_p = scipy_entropy(rho_p_prob + 1e-9)
# 
#         # Numerical derivative
#         dE = proxy_E_p - proxy_E
#         dS = proxy_S_p - proxy_S
# 
#         if dE == 0:
#             return 0.0 # Incompressible
# 
#         ic_score = float(dS / dE)
# 
#         if np.isnan(ic_score):
#             return 0.0
#         return ic_score
# 
#     except Exception as e:
#         print(f"[AletheiaMetrics] WARNING: IC calculation failed: {e}", file=sys.stderr)
#         return 0.0
# 
# # ---
# # SECTION 4: MAIN ORCHESTRATION (DRIVER HOOK)
# # ---
# 
# def main():
#     """
#     Main execution entry point for the SFP Module (v2.0).
#     Orchestrates the Quantule Profiler (CEPP), Provenance Kernel,
#     and Aletheia Metrics calculations.
#     """
#     parser = argparse.ArgumentParser(
#         description="Spectral Fidelity & Provenance (SFP) Module (Asset A6, v2.0)"
#     )
#     parser.add_argument(
#         "--input",
#         type=str,
#         required=True,
#         help="Path to the input rho_history.h5 data artifact."
#     )
#     parser.add_argument(
#         "--params",
#         type=str,
#         required=True,
#         help="Path to the parameters.json file for this run."
#     )
#     parser.add_argument(
#         "--output_dir",
#         type=str,
#         default=".",
#         help="Directory to save the provenance.json and atlas CSVs."
#     )
#     args = parser.parse_args()
# 
#     print(f"--- SFP Module (Asset A6, v2.0) Initiating Validation ---")
#     print(f"  Input Artifact: {args.input}")
#     print(f"  Params File:    {args.params}")
# 
#     # --- 1. Provenance Kernel (Hashing) ---
#     print("\n[1. Provenance Kernel]")
#     try:
#         with open(args.params, 'r') as f:
#             params_dict = json.load(f)
#     except Exception as e:
#         print(f"CRITICAL_FAIL: Could not load params file: {e}", file=sys.stderr)
#         sys.exit(1)
# 
#     config_hash = generate_canonical_hash(params_dict)
#     print(f"  Generated Canonical config_hash: {config_hash}")
#     param_hash_legacy = params_dict.get("param_hash_legacy", None)
# 
#     # --- 2. Fidelity Kernel (Quantule Profiler) ---
#     print("\n[2. Fidelity Kernel (CEPP v1.0)]")
# 
#     # Check for mock input file from previous tests
#     if args.input == "rho_history_mock.h5":
#         print("WARNING: Using 'rho_history_mock.h5'. This file is empty.")
#         print("Fidelity and Aletheia Metrics will be 0 or FAIL.")
#         # Create a dummy structure to allow the script to complete
#         profiler_run_results = {
#             "spectral_fidelity": {"validation_status": "FAIL: MOCK_INPUT", "log_prime_sse": 999.0},
#             "classification_results": {},
#             "raw_rho_final_state": np.zeros((16,16,16)) # Dummy shape
#         }
#     else:
#         # This is the normal execution path
#         if not os.path.exists(args.input):
#             print(f"CRITICAL_FAIL: Input file not found: {args.input}", file=sys.stderr)
#             sys.exit(1)
# 
#         profiler_run_results = run_quantule_profiler(args.input)
# 
#     spectral_fidelity_results = profiler_run_results["spectral_fidelity"]
#     classification_data = profiler_run_results["classification_results"]
#     rho_final = profiler_run_results["raw_rho_final_state"]
# 
#     print(f"  Validation Status: {spectral_fidelity_results['validation_status']}")
#     print(f"  Calculated SSE:    {spectral_fidelity_results['log_prime_sse']:.6f}")
# 
#     # --- 3. Aletheia Metrics (Phase 3 Implementation) ---
#     print("\n[3. Aletheia Coherence Metrics (Phase 3)]")
#     if rho_final is None or rho_final.size == 0:
#         print("  SKIPPING: No final state data to analyze.")
#         metrics_pcs, metrics_pli, metrics_ic = 0.0, 0.0, 0.0
#     else:
#         metrics_pcs = calculate_pcs(rho_final)
#         metrics_pli = calculate_pli(rho_final)
#         metrics_ic = calculate_ic(rho_final)
# 
#     print(f"  Phase Coherence Score (PCS): {metrics_pcs:.6f}")
#     print(f"  Principled Localization (PLI): {metrics_pli:.6f}")
#     print(f"  Informational Compressibility (IC): {metrics_ic:.6f}")
# 
#     # --- 4. Assemble & Save Canonical Artifacts ---
#     print("\n[4. Assembling Canonical Artifacts]")
# 
#     # A. Save Quantule Atlas CSV files
#     # The profiler returns a dict of {'filename': 'csv_content_string'}
#     atlas_paths = {}
#     for csv_name, csv_content in classification_data.items():
#         try:
#             # Save the CSV file, prefixed with the config_hash
#             csv_filename = f"{config_hash}_{csv_name}"
#             csv_path = os.path.join(args.output_dir, csv_filename)
#             with open(csv_path, 'w') as f:
#                 f.write(csv_content)
#             atlas_paths[csv_name] = csv_path
#             print(f"  Saved Quantule Atlas artifact: {csv_path}")
#         except Exception as e:
#             print(f"WARNING: Could not save Atlas CSV {csv_name}: {e}", file=sys.stderr)
# 
#     # B. Save the primary provenance.json artifact
#     provenance_artifact = {
#         "schema_version": SCHEMA_VERSION,
#         "config_hash": config_hash,
#         "param_hash_legacy": param_hash_legacy,
#         "execution_timestamp": datetime.now(timezone.utc).isoformat(),
#         "input_artifact_path": args.input,
# 
#         "spectral_fidelity": spectral_fidelity_results,
# 
#         "aletheia_metrics": {
#             "pcs": metrics_pcs,
#             "pli": metrics_pli,
#             "ic": metrics_ic
#         },
# 
#         "quantule_atlas_artifacts": atlas_paths,
# 
#         "secondary_metrics": {
#             "full_spectral_sse_tda": None # Deprecated
#         }
#     }
# 
#     output_filename = os.path.join(
#         args.output_dir,
#         f"provenance_{config_hash}.json"
#     )
# 
#     try:
#         with open(output_filename, 'w') as f:
#             json.dump(provenance_artifact, f, indent=2, sort_keys=True)
#         print(f"  SUCCESS: Saved primary artifact to {output_filename}")
#     except Exception as e:
#         print(f"CRITICAL_FAIL: Could not save artifact: {e}", file=sys.stderr)
#         sys.exit(1)
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile quantulemapper.py
# import numpy as np
# import os
# import tempfile
# 
# # Placeholder for LOG_PRIME_VALUES
# LOG_PRIME_VALUES = np.array([1.0, 2.0, 3.0, 4.0])
# 
# def analyze_4d(npy_file_path: str) -> dict:
#     """
#     MOCK function for the Quantule Profiler (CEPP v1.0).
#     It simulates the output expected by validation_pipeline.py.
#     """
#     print(f"[MOCK CEPP] Analyzing 4D data from: {npy_file_path}")
# 
#     try:
#         # Assuming the npy_file_path points to an actual .npy file
#         rho_history = np.load(npy_file_path)
#         print(f"[MOCK CEPP] Loaded dummy data of shape: {rho_history.shape}")
# 
#         total_sse = 0.485123 + np.random.rand() * 0.1 # Simulate a high SSE
#         validation_status = "FAIL: NO-LOCK" # As expected in the test description
#         scaling_factor_S = np.random.rand() * 10
#         dominant_peak_k = np.random.rand() * 5
# 
#         quantule_events_csv_content = (
#             "quantule_id,type,center_x,center_y,center_z,radius,magnitude\n"
#             "q1,TYPE_A,1.0,2.0,3.0,0.5,10.0\n"
#             "q2,TYPE_B,4.0,5.0,6.0,1.2,25.0\n"
#         )
# 
#         return {
#             "validation_status": validation_status,
#             "total_sse": total_sse,
#             "scaling_factor_S": scaling_factor_S,
#             "dominant_peak_k": dominant_peak_k,
#             "analysis_protocol": "CEPP v1.0 (MOCK)",
#             "csv_files": {
#                 "quantule_events.csv": quantule_events_csv_content
#             },
#         }
#     except Exception as e:
#         print(f"[MOCK CEPP] Error loading or processing dummy data: {e}", file=os.stderr)
#         return {
#             "validation_status": "FAIL: MOCK_ERROR",
#             "total_sse": 999.0,
#             "scaling_factor_S": 0.0,
#             "dominant_peak_k": 0.0,
#             "analysis_protocol": "CEPP v1.0 (MOCK)",
#             "csv_files": {},
#         }
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile aste_hunter.py
# #!/usr/bin/env python3
# 
# """
# aste_hunter.py
# CLASSIFICATION: Adaptive Learning Engine (ASTE V1.0)
# GOAL: Acts as the "Brain" of the ASTE. It reads validation reports
#       (provenance.json) from the SFP module, updates its internal
#       ledger, and applies an evolutionary algorithm to breed a
#       new generation of parameters that minimize the log_prime_sse.
# """
# 
# import os
# import json
# import csv
# import random
# import numpy as np
# from typing import Dict, Any, List, Optional
# import sys # Added for stderr
# import uuid # Added for temporary hash generation
# 
# # --- Configuration ---
# LEDGER_FILENAME = "simulation_ledger.csv"
# PROVENANCE_DIR = "provenance_reports" # Where the Validator saves reports
# SSE_METRIC_KEY = "log_prime_sse"
# HASH_KEY = "config_hash"
# 
# # Evolutionary Algorithm Parameters
# TOURNAMENT_SIZE = 3
# MUTATION_RATE = 0.1
# MUTATION_STRENGTH = 0.05
# 
# class Hunter:
#     """
#     Implements the core evolutionary "hunt" logic.
#     Manages a population of parameters stored in a ledger
#     and breeds new generations to minimize SSE.
#     """
# 
#     def __init__(self, ledger_file: str = LEDGER_FILENAME):
#         self.ledger_file = ledger_file
#         self.fieldnames = [
#             HASH_KEY,
#             SSE_METRIC_KEY,
#             "fitness",
#             "generation",
#             "param_kappa", # Example parameter 1
#             "param_sigma_k"  # Example parameter 2
#         ]
#         self.population = self._load_ledger()
#         print(f"[Hunter] Initialized. Loaded {len(self.population)} runs from {self.ledger_file}")
# 
#     def _load_ledger(self) -> List[Dict[str, Any]]:
#         """Loads the historical population from the CSV ledger."""
#         if not os.path.exists(self.ledger_file):
#             # Create an empty ledger if it doesn't exist
#             with open(self.ledger_file, 'w', newline='') as f:
#                 writer = csv.DictWriter(f, fieldnames=self.fieldnames)
#                 writer.writeheader()
#             return []
# 
#         population = []
#         try:
#             with open(self.ledger_file, 'r') as f:
#                 reader = csv.DictReader(f)
#                 for row in reader:
#                     # Convert numerical strings back to floats/ints
#                     for key in [SSE_METRIC_KEY, "fitness", "generation", "param_kappa", "param_sigma_k"]:
#                         if key in row and row[key]:
#                             try:
#                                 row[key] = float(row[key])
#                             except ValueError:
#                                 row[key] = None
#                     population.append(row)
#         except Exception as e:
#             print(f"[Hunter Error] Failed to load ledger: {e}", file=sys.stderr)
#         return population
# 
#     def _save_ledger(self):
#         """Saves the entire population back to the CSV ledger."""
#         try:
#             with open(self.ledger_file, 'w', newline='') as f:
#                 writer = csv.DictWriter(f, fieldnames=self.fieldnames, extrasaction='ignore')
#                 writer.writeheader()
#                 writer.writerows(self.population)
#         except Exception as e:
#             print(f"[Hunter Error] Failed to save ledger: {e}", file=sys.stderr)
# 
#     def process_generation_results(self, provenance_dir: str, job_hashes: List[str]):
#         """
#         MANDATE: Reads new provenance.json files, calculates fitness,
#         and updates the internal ledger.
#         """
#         print(f"[Hunter] Processing {len(job_hashes)} new results from {provenance_dir}...")
#         new_runs_processed = 0
#         for config_hash in job_hashes:
#             report_path = os.path.join(provenance_dir, f"provenance_{config_hash}.json")
# 
#             try:
#                 with open(report_path, 'r') as f:
#                     data = json.load(f)
# 
#                 # Extract the critical SSE metric
#                 sse = data["spectral_fidelity"][SSE_METRIC_KEY]
# 
#                 # Mandated Fitness Formula: fitness = 1 / SSE
#                 # Add a small epsilon to prevent division by zero
#                 fitness = 1.0 / (sse + 1e-9)
# 
#                 # Find the run in our population and update it
#                 found = False
#                 for run in self.population:
#                     if run[HASH_KEY] == config_hash:
#                         run[SSE_METRIC_KEY] = sse
#                         run["fitness"] = fitness
#                         found = True
#                         break
# 
#                 if found:
#                     new_runs_processed += 1
#                 else:
#                     print(f"[Hunter Warning] Hash {config_hash} found in JSON but not in population ledger.", file=sys.stderr)
# 
#             except FileNotFoundError:
#                 print(f"[Hunter Warning] Provenance file not found: {report_path}", file=sys.stderr)
#             except Exception as e:
#                 print(f"[Hunter Error] Failed to parse {report_path}: {e}", file=sys.stderr)
# 
#         print(f"[Hunter] Successfully processed and updated {new_runs_processed} runs.")
#         self._save_ledger()
# 
#     def _select_parent(self) -> Dict[str, Any]:
#         """Selects one parent using tournament selection."""
#         # Pick N random individuals from the population
#         tournament = random.sample(self.population, TOURNAMENT_SIZE)
# 
#         # The winner is the one with the highest fitness (lowest SSE)
#         winner = max(tournament, key=lambda x: x.get("fitness", 0.0))
#         return winner
# 
#     def _crossover(self, parent1: Dict[str, Any], parent2: Dict[str, Any]) -> Dict[str, Any]:
#         """Performs simple average crossover on parameters."""
#         child_params = {
#             "param_kappa": (parent1["param_kappa"] + parent2["param_kappa"]) / 2.0,
#             "param_sigma_k": (parent1["param_sigma_k"] + parent2["param_sigma_k"]) / 2.0,
#         }
#         return child_params
# 
#     def _mutate(self, params: Dict[str, Any]) -> Dict[str, Any]:
#         """Applies random mutation to parameters."""
#         if random.random() < MUTATION_RATE:
#             params["param_kappa"] += np.random.normal(0, MUTATION_STRENGTH)
#             # Ensure parameters stay within reasonable bounds
#             params["param_kappa"] = max(0.001, params["param_kappa"])
# 
#         if random.random() < MUTATION_RATE:
#             params["param_sigma_k"] += np.random.normal(0, MUTATION_STRENGTH)
#             params["param_sigma_k"] = max(0.1, params["param_sigma_k"])
# 
#         return params
# 
#     def get_next_generation(self, population_size: int) -> List[Dict[str, Any]]:
#         """
#         Breeds a new generation of parameters.
#         This is the main function called by the Orchestrator.
#         """
#         new_generation_params = []
# 
#         if not self.population:
#             # Generation 0: Create a random population
#             print("[Hunter] No population found. Generating random Generation 0.")
#             for _ in range(population_size):
#                 params = {
#                     "param_kappa": np.random.uniform(0.01, 0.1),
#                     "param_sigma_k": np.random.uniform(0.1, 1.0)
#                 }
#                 new_generation_params.append(params)
#         else:
#             # Breed a new generation from the existing population
#             print(f"[Hunter] Breeding Generation {self.population[-1]['generation'] + 1}...")
#             # Sort by fitness (highest first)
#             sorted_population = sorted(self.population, key=lambda x: x.get("fitness", 0.0), reverse=True)
# 
#             # Elitism: Keep the top 2 best individuals
#             new_generation_params.append({"param_kappa": sorted_population[0]["param_kappa"], "param_sigma_k": sorted_population[0]["param_sigma_k"]})
#             new_generation_params.append({"param_kappa": sorted_population[1]["param_kappa"], "param_sigma_k": sorted_population[1]["param_sigma_k"]})
# 
#             # Breed the rest
#             for _ in range(population_size - 2):
#                 parent1 = self._select_parent()
#                 parent2 = self._select_parent()
#                 child_params = self._crossover(parent1, parent2)
#                 mutated_child_params = self._mutate(child_params)
#                 new_generation_params.append(mutated_child_params)
# 
#         # --- Update Internal Ledger ---
#         # Add these new jobs to our internal population *before* they run
#         # They will be updated with SSE/fitness later.
#         current_gen = self.population[-1]['generation'] + 1 if self.population else 0
#         new_jobs = []
#         for params in new_generation_params:
#             # This is a temporary hash, as the Orchestrator will add a UUID
#             # The *real* hash will be in the provenance.json file
#             temp_hash = f"temp_job_{uuid.uuid4().hex[:10]}"
#             job_entry = {
#                 HASH_KEY: temp_hash,
#                 SSE_METRIC_KEY: None,
#                 "fitness": None,
#                 "generation": current_gen,
#                 "param_kappa": params["param_kappa"],
#                 "param_sigma_k": params["param_sigma_k"]
#             }
#             new_jobs.append(job_entry)
# 
#         # We will add these to the population *after* the Orchestrator
#         # provides the real hashes. This function just returns the raw params.
# 
#         # We need to return the parameters *and* update our internal state
#         # The orchestrator will give us the *real* hashes
# 
#         # Let's simplify: The Hunter only returns params.
#         # The Orchestrator adds them to the ledger with the *real* hashes.
#         # This is a cleaner separation of concerns.
#         # --> We will adjust this in the Orchestrator test.
# 
#         # For this test, we'll keep the logic simple as above.
#         # The Orchestrator will call `process_generation_results`
#         # We need a way to link the new jobs.
# 
#         # Let's stick to the prompt's architecture:
#         # 1. Hunter returns new params.
#         # 2. Orchestrator creates hashes, saves configs, runs jobs, saves provenance.
#         # 3. Orchestrator calls `process_generation_results` with the *list of hashes* it created.
# 
#         # This means the Hunter *doesn't* add them to the ledger.
#         # The Orchestrator must pass the params *back* to the hunter.
#         # Let's create a new function for that.
# 
#         self.last_generation_jobs = [] # Clear last batch
#         for params in new_generation_params:
#             # This job entry is temporary, to be confirmed by the orchestrator
#             job_entry = {
#                 SSE_METRIC_KEY: None,
#                 "fitness": None,
#                 "generation": current_gen,
#                 "param_kappa": params["param_kappa"],
#                 "param_sigma_k": params["param_sigma_k"]
#             }
#             self.last_generation_jobs.append(job_entry)
# 
#         return new_generation_params # Return raw params to Orchestrator
# 
#     def register_new_jobs(self, job_list: List[Dict[str, Any]]):
#         """
#         Called by the Orchestrator *after* it has generated
#         canonical hashes for the new jobs.
#         """
#         self.population.extend(job_list)
#         print(f"[Hunter] Registered {len(job_list)} new jobs in ledger.")
# 
#     def get_best_run(self) -> Optional[Dict[str, Any]]:
#         """
#         Utility to get the best-performing run from the ledger.
#         """
#         if not self.population:
#             return None
#         valid_runs = [r for r in self.population if r.get("fitness")]
#         if not valid_runs:
#             return None
#         return max(valid_runs, key=lambda x: x["fitness"])
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile worker_v7.py
# #!/usr/bin/env python3
# 
# """
# worker_v7.py
# CLASSIFICATION: Simulation Worker (ASTE V1.0)
# GOAL: Executes a single, JAX-based, JIT-compiled simulation run.
#       This component is architected to be called by the
#       adaptive_hunt_orchestrator.py and is the core physics engine.
#       It adheres to the jax.lax.scan HPC mandate.
# """
# 
# import jax
# import jax.numpy as jnp
# import numpy as np # For initial setup
# import h5py
# import json
# import os
# import sys
# import argparse
# from typing import NamedTuple, Tuple, Dict, Any
# from functools import partial
# 
# # ---
# # SECTION 1: JAX STATE AND PHYSICS DEFINITIONS
# # ---
# 
# class FMIAState(NamedTuple):
#     """
#     JAX Pytree for the core FMIA state (Field Dynamics).
#     This is the state evolved by the RK4 integrator.
#     """
#     rho: jnp.ndarray  # Resonance Density
#     pi: jnp.ndarray   # Conjugate Momentum (d_rho_dt)
# 
# class SimState(NamedTuple):
#     """
#     The unified "carry" state for the jax.lax.scan loop.
#     MANDATE: Includes k_vectors as dynamic state to ensure JIT integrity.
#     """
#     fmia_state: FMIAState     # The evolving physics fields
#     g_munu: jnp.ndarray       # The evolving metric tensor
#     Omega: jnp.ndarray        # The evolving conformal factor
#     k_vectors: Tuple[jnp.ndarray, ...] # (kx, ky, kz) grids
#     k_squared: jnp.ndarray    # |k|^2 grid
# 
# 
# # ---
# # SECTION 2: PHYSICS KERNELS (GAPS & PROXIES)
# # ---
# 
# @jax.jit
# def jnp_effective_conformal_factor(
#     rho: jnp.ndarray,
#     coupling_alpha: float,
#     epsilon: float = 1e-9
# ) -> jnp.ndarray:
#     """
#     [ECM Model Core] Computes the Effective Conformal Factor Omega(rho).
#     Model: Omega = exp[ alpha * (rho - 1) ].
#     This is the computable proxy that solves the "Gravity Gap".
#     """
#     alpha = jnp.maximum(coupling_alpha, epsilon)
#     rho_fluctuation = rho - 1.0 # Fluctuation from vacuum (rho=1)
#     Omega = jnp.exp(alpha * rho_fluctuation)
#     return Omega
# 
# @jax.jit
# def jnp_construct_conformal_metric(
#     rho: jnp.ndarray,
#     coupling_alpha: float
# ) -> Tuple[jnp.ndarray, jnp.ndarray]:
#     """
#     Constructs the 4x4xNxNxN spacetime metric g_munu = Omega^2 * eta_munu.
#     This function closes the geometric loop using the ECM proxy model.
#     """
#     # 1. Calculate the Effective Conformal Factor
#     Omega = jnp_effective_conformal_factor(rho, coupling_alpha)
#     Omega_sq = jnp.square(Omega)
# 
#     # 2. Construct the Conformal Metric: g_munu = Omega^2 * eta_munu
#     grid_shape = rho.shape
#     g_munu = jnp.zeros((4, 4) + grid_shape)
# 
#     # g_00 = -Omega^2 (Time component)
#     g_munu = g_munu.at[0, 0, ...].set(-Omega_sq)
# 
#     # g_ii = Omega^2 (Spatial components)
#     g_munu = g_munu.at[1, 1, ...].set(Omega_sq) # g_xx
#     g_munu = g_munu.at[2, 2, ...].set(Omega_sq) # g_yy
#     g_munu = g_munu.at[3, 3, ...].set(Omega_sq) # g_zz
# 
#     return g_munu, Omega
# 
# @jax.jit
# def jnp_metric_aware_laplacian(
#     rho: jnp.ndarray,
#     Omega: jnp.ndarray,
#     k_squared: jnp.ndarray,
#     k_vectors: Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]
# ) -> jnp.ndarray:
#     """
#     PLACEHOLDER for the Covariant Laplace-Beltrami operator (square_g).
#     Implements the 3D formula: Delta_g(rho) = O^-2 * [ (nabla^2 rho) + O^-1 * (nabla O . nabla rho) ]
#     """
#     kx_3d, ky_3d, kz_3d = k_vectors
#     Omega_inv = 1.0 / (Omega + 1e-9)
#     Omega_sq_inv = Omega_inv**2
# 
#     # --- 1. Compute flat-space Laplacian: nabla^2(rho) ---
#     rho_k = jnp.fft.fftn(rho)
#     laplacian_rho_k = -k_squared * rho_k
#     laplacian_rho = jnp.fft.ifftn(laplacian_rho_k).real
# 
#     # --- 2. Compute flat-space gradients: nabla(rho) and nabla(Omega) ---
#     grad_rho_x = jnp.fft.ifftn(1j * kx_3d * rho_k).real
#     grad_rho_y = jnp.fft.ifftn(1j * ky_3d * rho_k).real
#     grad_rho_z = jnp.fft.ifftn(1j * kz_3d * rho_k).real
# 
#     Omega_k = jnp.fft.fftn(Omega)
#     grad_Omega_x = jnp.fft.ifftn(1j * kx_3d * Omega_k).real
#     grad_Omega_y = jnp.fft.ifftn(1j * ky_3d * Omega_k).real
#     grad_Omega_z = jnp.fft.ifftn(1j * kz_3d * Omega_k).real
# 
#     # --- 3. Compute dot product: (nabla O . nabla rho) ---
#     nabla_dot_product = (grad_Omega_x * grad_rho_x +
#                          grad_Omega_y * grad_rho_y +
#                          grad_Omega_z * grad_rho_z)
# 
#     # --- 4. Assemble the final formula ---
#     term1 = laplacian_rho
#     term2 = Omega_inv * nabla_dot_product
#     Delta_g_rho = Omega_sq_inv * (term1 + term2)
# 
#     return Delta_g_rho
# 
# @jax.jit
# def jnp_get_derivatives(
#     state: FMIAState,
#     t: float,
#     k_squared: jnp.ndarray,
#     k_vectors: Tuple[jnp.ndarray, ...],
#     g_munu: jnp.ndarray,
#     Omega: jnp.ndarray,
#     constants: Dict[str, float]
# ) -> FMIAState:
#     """
#     Calculates the time derivatives (d_rho_dt, d_pi_dt) for the
#     Metric-Aware FMIA EOM (Equation of Motion).
#     """
#     rho, pi = state.rho, state.pi
# 
#     # --- 1. Physics Calculations ---
# 
#     # CRITICAL: Replace flat Laplacian with Metric-Aware Laplacian
#     laplacian_g_rho = jnp_metric_aware_laplacian(
#         rho, Omega, k_squared, k_vectors
#     )
# 
#     # Placeholder for Potential Term (V'(rho))
#     V_prime = rho - rho**3 # Example: phi^4 potential derivative
# 
#     # GAP: Non-Local "Splash" Term (G_non_local)
#     # This is a known physics gap (Part V.B) and is zeroed out.
#     G_non_local_term = jnp.zeros_like(pi)
# 
#     # --- 2. Calculate Time Derivatives ---
#     d_rho_dt = pi
# 
#     d_pi_dt = (
#         constants.get('D', 1.0) * laplacian_g_rho +  # <-- METRIC-AWARE
#         V_prime +
#         G_non_local_term +                         # <-- GAP
#         -constants.get('eta', 0.1) * pi            # Damping
#     )
# 
#     return FMIAState(rho=d_rho_dt, pi=d_pi_dt)
# 
# @partial(jax.jit, static_argnames=['derivs_func'])
# def rk4_step(
#     derivs_func: callable,
#     state: FMIAState,
#     t: float,
#     dt: float,
#     k_squared: jnp.ndarray,
#     k_vectors: Tuple[jnp.ndarray, ...],
#     g_munu: jnp.ndarray,
#     Omega: jnp.ndarray,
#     constants: Dict[str, float]
# ) -> FMIAState:
#     """
#     Performs a single Runge-Kutta 4th Order (RK4) step.
#     """
# 
#     k1 = derivs_func(state, t, k_squared, k_vectors, g_munu, Omega, constants)
# 
#     state_k2 = jax.tree_util.tree_map(lambda y, dy: y + 0.5 * dt * dy, state, k1)
#     k2 = derivs_func(state_k2, t + 0.5 * dt, k_squared, k_vectors, g_munu, Omega, constants)
# 
#     state_k3 = jax.tree_util.tree_map(lambda y, dy: y + 0.5 * dt * dy, state, k2)
#     k3 = derivs_func(state_k3, t + 0.5 * dt, k_squared, k_vectors, g_munu, Omega, constants)
# 
#     state_k4 = jax.tree_util.tree_map(lambda y, dy: y + dt * dy, state, k3)
#     k4 = derivs_func(state_k4, t + dt, k_squared, k_vectors, g_munu, Omega, constants)
# 
#     next_state = jax.tree_util.tree_map(
#         lambda y, dy1, dy2, dy3, dy4: y + (dt / 6.0) * (dy1 + 2.0*dy2 + 2.0*dy3 + dy4),
#         state, k1, k2, k3, k4
#     )
# 
#     return next_state
# 
# # ---
# # SECTION 3: JAX.LAX.SCAN ORCHESTRATOR (HPC MANDATE)
# # ---
# 
# @partial(jax.jit, static_argnames=['dt', 'D_const', 'eta_const', 'kappa_const', 'sigma_k_const', 'alpha_coupling'])
# def simulation_step(
#     carry_state: SimState,
#     t: float,
#     dt: float,
#     D_const: float,
#     eta_const: float,
#     kappa_const: float,
#     sigma_k_const: float,
#     alpha_coupling: float
# ) -> Tuple[SimState, jnp.ndarray]:
#     """
#     Executes one full, JIT-compiled step of the co-evolutionary loop.
#     This is the function body for jax.lax.scan.
#     """
#     # 1. Unpack all state variables from the carry
#     current_fmia_state = carry_state.fmia_state
#     current_g_munu = carry_state.g_munu
#     current_Omega = carry_state.Omega
#     k_vectors = carry_state.k_vectors
#     k_squared = carry_state.k_squared
# 
#     # Reconstruct constants dictionary for functions expecting it
#     constants_dict = {
#         'D': D_const,
#         'eta': eta_const,
#         'kappa': kappa_const,
#         'sigma_k': sigma_k_const
#     }
# 
#     # --- STAGE 1: FIELD EVOLUTION (rho_n -> rho_n+1) ---
#     # Field evolves on the *current* geometry
#     next_fmia_state = rk4_step(
#         jnp_get_derivatives,
#         current_fmia_state,
#         t, dt,
#         k_squared, k_vectors,
#         current_g_munu, current_Omega,
#         constants_dict
#     )
# 
#     # --- STAGE 2 & 3: SOURCE & GEOMETRY (rho_n+1 -> g_munu_n+1) ---
#     # New geometry is calculated from the *new* field state
#     next_g_munu, next_Omega = jnp_construct_conformal_metric(
#         next_fmia_state.rho,
#         alpha_coupling
#     )
# 
#     # 4. Assemble NEW Carry State (Closing the Loop)
#     new_carry = SimState(
#         fmia_state=next_fmia_state,
#         g_munu=next_g_munu,
#         Omega=next_Omega,
#         k_vectors=k_vectors,  # Constants are passed through
#         k_squared=k_squared
#     )
# 
#     # Return (new_carry, data_to_log)
#     # We log the rho field for this step
#     return new_carry, new_carry.fmia_state.rho
# 
# def run_simulation(N_grid: int, L_domain: float, T_steps: int, ALPHA: float, KAPPA: float, SIGMA_K: float, D: float, ETA: float) -> Tuple[SimState, jnp.ndarray]:
#     """
#     Main JAX driver function. Sets up and runs the jax.lax.scan loop.
#     """
# 
#     # 1. Precompute JAX k-vectors (Non-hashable)
#     k_1D = 2 * jnp.pi * jnp.fft.fftfreq(N_grid, d=L_domain/N_grid)
#     kx_3d, ky_3d, kz_3d = jnp.meshgrid(k_1D, k_1D, k_1D, indexing='ij')
#     k_vectors_tuple = (kx_3d, ky_3d, kz_3d)
#     k_squared_array = kx_3d**2 + ky_3d**2 + kz_3d**2
# 
#     # 2. Initialize the SimState (Bundling k-vectors into the 'carry')
#     # Use a small random noise to break symmetry
#     key = jax.random.PRNGKey(42)
#     initial_rho = jnp.ones((N_grid, N_grid, N_grid)) + jax.random.uniform(key, (N_grid, N_grid, N_grid)) * 0.01
#     initial_pi = jnp.zeros_like(initial_rho)
# 
#     initial_fmia_state = FMIAState(rho=initial_rho, pi=initial_pi)
# 
#     initial_g_munu, initial_Omega = jnp_construct_conformal_metric(
#         initial_rho, ALPHA
#     )
# 
#     initial_carry = SimState(
#         fmia_state=initial_fmia_state,
#         g_munu=initial_g_munu,
#         Omega=initial_Omega,
#         k_vectors=k_vectors_tuple,
#         k_squared=k_squared_array
#     )
# 
#     # 3. Define the main scan body function
#     scan_fn = partial(
#         simulation_step,
#         dt=0.01, # DT is fixed for now
#         D_const=D,
#         eta_const=ETA,
#         kappa_const=KAPPA,
#         sigma_k_const=SIGMA_K,
#         alpha_coupling=ALPHA
#     )
# 
#     # 4. Execute the fully JIT-compiled loop
#     timesteps = jnp.arange(T_steps) * 0.01 # DT is fixed for now
# 
#     final_carry, history = jax.lax.scan(
#         scan_fn,
#         initial_carry,
#         timesteps,
#         length=T_steps
#     )
# 
#     return final_carry, history # history is rho_history
# 
# # ---
# # SECTION 4: MAIN ORCHESTRATOR (DRIVER HOOK)
# # ---
# 
# def main():
#     """
#     Main entry point for the Worker.
#     Called by adaptive_hunt_orchestrator.py.
#     """
#     parser = argparse.ArgumentParser(
#         description="ASTE Simulation Worker (worker_v7.py)"
#     )
#     parser.add_argument(
#         "--params",
#         type=str,
#         required=True,
#         help="Path to the parameters.json file for this job."
#     )
#     parser.add_argument(
#         "--output",
#         type=str,
#         required=True,
#         help="Path to the output rho_history.h5 data artifact."
#     )
#     args = parser.parse_args()
# 
#     print(f"[Worker] Job started. Loading config: {args.params}")
# 
#     # --- 1. Load Parameters ---
#     try:
#         with open(args.params, 'r') as f:
#             params = json.load(f)
# 
#         # Extract simulation and physics parameters
#         # Using .get() for safety, with defaults
#         sim_params = params.get("simulation", {})
#         phys_params = params.get("physics", {})
# 
#         N_GRID = sim_params.get("N_grid", 32)
#         L_DOMAIN = sim_params.get("L_domain", 10.0)
#         T_STEPS = sim_params.get("T_steps", 100)
#         # DT is fixed in run_simulation now, not passed from params
#         # DT = sim_params.get("dt", 0.01)
# 
#         ALPHA = phys_params.get("alpha", 0.5)
#         # Parameters from Hunter (Part III.B)
#         KAPPA = params.get("param_kappa", 0.007) # Default to a known good value
#         SIGMA_K = params.get("param_sigma_k", 0.55) # Default
# 
#         D = phys_params.get("D", 1.0)
#         ETA = phys_params.get("eta", 0.1)
# 
#     except Exception as e:
#         print(f"[Worker Error] Failed to load or parse params file: {e}", file=sys.stderr)
#         sys.exit(1)
# 
#     print(f"[Worker] Parameters loaded: N={N_GRID}, Steps={T_STEPS}, K_NonLocal={KAPPA}")
# 
#     # --- 2. Run Simulation ---
#     print("[Worker] JAX: Compiling and running simulation loop...")
#     try:
#         final_carry, rho_history = run_simulation(
#             N_grid=N_GRID,
#             L_domain=L_DOMAIN,
#             T_steps=T_STEPS,
#             # DT is fixed in run_simulation now
#             ALPHA=ALPHA,
#             KAPPA=KAPPA,
#             SIGMA_K=SIGMA_K,
#             D=D,
#             ETA=ETA
#         )
#         print("[Worker] Simulation complete.")
# 
#     except Exception as e:
#         print(f"[Worker Error] JAX simulation failed: {e}", file=sys.stderr)
#         sys.exit(1)
# 
#     # --- 3. Save Artifact ---
#     print(f"[Worker] Saving artifact to: {args.output}")
#     try:
#         # Convert JAX array back to NumPy for HDF5 I/O
#         rho_history_np = np.asarray(rho_history)
# 
#         with h5py.File(args.output, 'w') as f:
#             f.create_dataset(
#                 'rho_history',
#                 data=rho_history_np,
#                 chunks=(1, N_GRID, N_GRID, N_GRID), # Chunked by timestep
#                 compression="gzip"
#             )
#             # Save parameters as metadata attributes
#             f.attrs['config_hash'] = params.get('config_hash', 'unknown')
#             f.attrs['param_kappa'] = KAPPA
#             f.attrs['param_sigma_k'] = SIGMA_K
# 
#         print("[Worker] SUCCESS: Artifact saved.")
# 
#     except Exception as e:
#         print(f"CRITICAL_FAIL: Could not save HDF5 artifact: {e}", file=sys.stderr)
#         sys.exit(1)
# 
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile adaptive_hunt_orchestrator.py
# #!/usr/bin/env python3
# 
# """
# adaptive_hunt_orchestrator.py
# CLASSIFICATION: Master Driver (ASTE V1.0)
# GOAL: Manages the entire end-to-end simulation lifecycle. This script
#        bootstraps the system, calls the Hunter for parameters, launches
#       the Worker to simulate, and initiates the Validator (SFP module)
#       to certify the results, closing the adaptive loop.
# """
# 
# import os
# import json
# import subprocess
# import sys
# import uuid
# from typing import Dict, Any, List
# 
# # --- Import Shared Components ---
# # We import the Provenance Kernel from the SFP module to generate
# # the canonical hash. This is a critical architectural link.
# try:
#     from validation_pipeline import generate_canonical_hash
# except ImportError:
#     print("Error: Could not import 'generate_canonical_hash'.", file=sys.stderr)
#     print("Please ensure 'validation_pipeline.py' is in the same directory.", file=sys.stderr)
#     sys.exit(1)
# 
# # We also import the "Brain" of the operation
# try:
#     import aste_hunter
# except ImportError:
#     print("Error: Could not import 'aste_hunter'.", file=sys.stderr)
#     print("Please ensure 'aste_hunter.py' is in the same directory.", file=sys.stderr)
#     sys.exit(1)
# 
# 
# # --- Configuration ---
# # These paths define the ecosystem's file structure
# CONFIG_DIR = "input_configs"
# DATA_DIR = "simulation_data"
# PROVENANCE_DIR = "provenance_reports"
# WORKER_SCRIPT = "worker_v7.py" # The JAX-based worker
# VALIDATOR_SCRIPT = "validation_pipeline.py" # The SFP Module
# 
# # --- Test Parameters ---
# # Use small numbers for a quick test run
# NUM_GENERATIONS = 2     # Run 2 full loops (Gen 0, Gen 1)
# POPULATION_SIZE = 4    # Run 4 simulations per generation
# 
# 
# def setup_directories():
#     """Ensures all required I/O directories exist."""
#     os.makedirs(CONFIG_DIR, exist_ok=True)
#     os.makedirs(DATA_DIR, exist_ok=True)
#     os.makedirs(PROVENANCE_DIR, exist_ok=True)
#     print(f"Orchestrator: I/O directories ensured: {CONFIG_DIR}, {DATA_DIR}, {PROVENANCE_DIR}")
# 
# def run_simulation_job(config_hash: str, params_filepath: str) -> bool:
#     """
#     Executes a single end-to-end simulation job (Worker + Validator).
#     This function enforces the mandated workflow.
#     """
#     print(f"\n--- ORCHESTRATOR: STARTING JOB {config_hash[:10]}... ---")
# 
#     # Define file paths based on the canonical hash
#     # This enforces the "unbreakable cryptographic link"
#     rho_history_path = os.path.join(DATA_DIR, f"rho_history_{config_hash}.h5")
# 
#     try:
#         # --- 3. Execution Step (Simulation) ---
#         print(f"[Orchestrator] -> Calling Worker: {WORKER_SCRIPT}")
#         worker_command = [
#             "python", WORKER_SCRIPT,
#             "--params", params_filepath,
#             "--output", rho_history_path
#         ]
# 
#         # We use subprocess.run() which waits for the command to complete.
#         # This is where the JAX compilation will happen on the first run.
#         worker_process = subprocess.run(worker_command, check=False, capture_output=True, text=True)
# 
#         if worker_process.returncode != 0:
#             print(f"ERROR: [JOB {config_hash[:10]}] WORKER FAILED.", file=sys.stderr)
#             print(f"COMMAND: {' '.join(worker_process.args)}", file=sys.stderr)
#             print(f"STDOUT: {worker_process.stdout}", file=sys.stderr)
#             print(f"STDERR: {worker_process.stderr}", file=sys.stderr)
#             return False
# 
#         print(f"[Orchestrator] <- Worker {config_hash[:10]} OK.")
# 
#         # --- 4. Fidelity Step (Validation) ---
#         print(f"[Orchestrator] -> Calling Validator: {VALIDATOR_SCRIPT}")
#         validator_command = [
#             "python", VALIDATOR_SCRIPT,
#             "--input", rho_history_path,
#             "--params", params_filepath,
#             "--output_dir", PROVENANCE_DIR
#         ]
#         validator_process = subprocess.run(validator_command, check=False, capture_output=True, text=True)
# 
#         if validator_process.returncode != 0:
#             print(f"ERROR: [JOB {config_hash[:10]}] VALIDATOR FAILED.", file=sys.stderr)
#             print(f"COMMAND: {' '.join(validator_process.args)}", file=sys.stderr)
#             print(f"STDOUT: {validator_process.stdout}", file=sys.stderr)
#             print(f"STDERR: {validator_process.stderr}", file=sys.stderr)
#             return False
# 
#         print(f"[Orchestrator] <- Validator {config_hash[:10]} OK.")
# 
#         print(f"--- ORCHESTRATOR: JOB {config_hash[:10]} SUCCEEDED ---")
#         return True
# 
#     except FileNotFoundError as e:
#         print(f"ERROR: [JOB {config_hash[:10]}] Script not found: {e.filename}", file=sys.stderr)
#         return False
#     except Exception as e:
#         print(f"ERROR: [JOB {config_hash[:10]}] An unexpected error occurred: {e}", file=sys.stderr)
#         return False
# 
# def main():
#     """
#     Main entry point for the Adaptive Simulation Steering Engine (ASTE).
#     """
#     print("--- ASTE ORCHESTRATOR V1.0 [BOOTSTRAP] ---")
#     setup_directories()
# 
#     # 1. Bootstrap: Initialize the Hunter "Brain"
#     hunter = aste_hunter.Hunter(ledger_file="simulation_ledger.csv")
# 
#     # --- MAIN ORCHESTRATION LOOP ---
#     for gen in range(NUM_GENERATIONS):
#         print(f"\n========================================================")
#         print(f"    ASTE ORCHESTRATOR: STARTING GENERATION {gen}")
#         print(f"========================================================")
# 
#         # 2. Get Tasks: Hunter breeds the next generation of parameters
#         parameter_batch = hunter.get_next_generation(POPULATION_SIZE)
# 
#         jobs_to_run = []
# 
#         # --- 2a. Provenance & Registration Step ---
#         print(f"[Orchestrator] Registering {len(parameter_batch)} new jobs for Gen {gen}...")
#         for params_dict in parameter_batch:
# 
#             # Create a temporary dictionary for hashing that does NOT include run_uuid or config_hash
#             # This ensures the canonical hash is always derived only from core simulation parameters.
#             params_for_hashing = params_dict.copy()
#             params_for_hashing.pop('config_hash', None) # Remove if present
#             params_for_hashing.pop('run_uuid', None) # Remove if present
# 
#             # Generate the canonical hash (Primary Key) from the core parameters
#             config_hash = generate_canonical_hash(params_for_hashing)
# 
#             # Now add metadata to the params_dict that will be saved to disk.
#             # The canonical config_hash should be part of the saved parameters
#             # for the worker to attribute its output. run_uuid is for unique instance tracking.
#             params_dict['config_hash'] = config_hash
#             params_dict['run_uuid'] = str(uuid.uuid4()) # Add a unique ID to distinguish identical parameter sets
# 
#             # --- 2b. Save Config ---
#             params_filepath = os.path.join(CONFIG_DIR, f"config_{config_hash}.json")
#             try:
#                 with open(params_filepath, 'w') as f:
#                     json.dump(params_dict, f, indent=2, sort_keys=True)
#             except Exception as e:
#                 print(f"ERROR: Could not write config file {params_filepath}. {e}", file=sys.stderr)
#                 continue # Skip this job
# 
#             # --- 2c. Register Job with Hunter ---
#             job_entry = {
#                 aste_hunter.HASH_KEY: config_hash,
#                 "generation": gen,
#                 "param_kappa": params_dict["param_kappa"],
#                 "param_sigma_k": params_dict["param_sigma_k"],
#                 "params_filepath": params_filepath
#             }
#             jobs_to_run.append(job_entry)
# 
#         # Register the *full* batch with the Hunter's ledger
#         hunter.register_new_jobs(jobs_to_run)
# 
#         # --- 3 & 4. Execute Batch Loop (Worker + Validator) ---
#         job_hashes_completed = []
#         for job in jobs_to_run:
#             success = run_simulation_job(
#                 config_hash=job[aste_hunter.HASH_KEY],
#                 params_filepath=job["params_filepath"]
#             )
#             if success:
#                 job_hashes_completed.append(job[aste_hunter.HASH_KEY])
# 
#         # --- 5. Ledger Step (Cycle Completion) ---
#         print(f"\n[Orchestrator] GENERATION {gen} COMPLETE.")
#         print("[Orchestrator] Notifying Hunter to process results...")
#         hunter.process_generation_results(
#             provenance_dir=PROVENANCE_DIR,
#             job_hashes=job_hashes_completed
#         )
# 
#         best_run = hunter.get_best_run()
#         if best_run:
#             print(f"[Orch] Best Run So Far: {best_run[aste_hunter.HASH_KEY][:10]}... (SSE: {best_run[aste_hunter.SSE_METRIC_KEY]:.6f})")
# 
#     print("\n========================================================")
#     print("--- ASTE ORCHESTRATOR: ALL GENERATIONS COMPLETE ---")
#     print("========================================================")
# 
# if __name__ == "__main__":
#     main()

# Step 4: Execute the Full System
!python adaptive_hunt_orchestrator.py

